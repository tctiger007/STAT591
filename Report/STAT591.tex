\documentclass[11pt]{report}
\usepackage{./assignment}
\usepackage{slashbox}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage{stmaryrd}
\usepackage[final]{pdfpages}
\usepackage{array}
\usepackage{multirow}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{authblk}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{epstopdf}

\input{./Definitions}

\begin{document}
\title{
  \huge STAT 591 Summary Report \\ 
  \vspace{10mm}
  \large  Functional Data Analysis for Sparse Longitudinal Data\\
  \normalsize Fang Yao, Hans-Georg M\"{u}ller \& Jane-Ling Wang}

\author{Wangfei Wang \\ wwang75@uic.edu }

\graphicspath{./Figures/}

\maketitle

\section{SUMMARY OF CONTRIBUTIONS}
Functional principal components (FPC) analysis can reduce random trajectories of random curves to a set of FPC scores, and therefore is popular in longitudinal data analysis. 
For a given sample of random trajectories, FPC analysis characterizes the dominant mode of variation  around an overall mean trend function. 

In longitudinal data analysis, there are extensive research of FPC analysis on repeated measures at a dense grid of regularly spaced time points. 
However, it is not uncommon that repeated measurements are infrequent, and these measurements are irregularly spaced per subject. 
In this situation, FPC analysis has limitations because of the sparse repeated measurements. 
The authors summarized a few available models that can be applied to irregular grid of time measurements. 
For example, kernel-based functional principal components analysis for repeated measurements with irregular time points was proposed by Staniswalis and Lee [refs]. %%%%%\cite{}. 
Their method was further studied by other groups [refs]; %%%%%\\cite{}; 
however, when the measurement time points vary widely across individuals and the measurements per subject is very sparse (i.e., one or two measurements per subject), the FPC scores cannot be approximated by the usual integration method. 
Some other groups proposed that by using linear mixed models or reduced-rank mixed effects models, they could use B-splines to model the individual curves with random coefficients [refs]. %%%%%\cite{}.
But because of the complexity of the models, the asymptotic properties of the estimated components were not investigated. 

Taken together, the authors proposed a simpler and more straightforward method to determine eigenfunctions, which they represent the trajectories directly using the Karhunen-Lo\`{e}ve expansion.
Their contributions include:
\begin{itemize}
	\item They proposed a version of functional principal components (FPC) analysis, in which they framed the FPC scores as conditional expectations.
	And thus they coined this method ``principal components analysis through conditional expectation (PACE)''.
	\item In the model, they took into account the additional measurement errors. 
	\item They derived the asymptotic consistency properties.
	\item They derived the asymptotic distribution needed for obtaining point-wise confidence intervals for individual trajectories. 
\end{itemize}

\section{INNOVATION} 
\begin{itemize}
	\item The proposed conditional model is designed for sparse and irregular longitudinal data. 
	\item Under Gaussian assumptions, the authors showed that estimation of individual FPC scores are the best prediction; and under non-Gaussian assumption, they provide estimates for best linear prediction. 
	\item One-curve-leave-out cross-validation was proposed to choose auxiliary parameters. 
	\item Akaike information criterion (AIC) was used for faster computation to select eigenfunctions. 
\end{itemize}

\section{FUNCTIONAL PRINCIPAL COMPONENTS ANALYSIS FOR SPARSE DATA}

\subsection*{Model with Measurement Errors}
The authors modeled the sparse functional data as noisy sampled points from trajectories. 
These trajectories are assumed independent realizations of a smooth random function with unknown mean $\expec{X(t)} = \mu(t)$ and covariance function $cov(X(s), X(t)) = G(s,t)$, where domain of X($\cdot$) is $\mathcal{T}$.
It was assumed that G has an orthogonal expansion in terms of eigenfunction $\phi_k$ and eigenvalues $\lambda_k$: $G(s,t) = \sum_k{\lambda_k\phi_k(s)\phi_k(t)}, \; \; t, s \in \mathcal{T}$, where $\lambda_1 \geq \lambda_2 \geq \cdot \cdot \cdot$.
Assuming $Y_{ij}$ is the jth observation of the random function X($\cdot$) made at a random time $T_{ij}$ and let $\epsilon_{ij}$ be the measurement errors that are iid and are independent of random coefficients $\xi_{ik}$, where $i = 1, ..., n; j = 1, ..., N_i; k = 1, 2, ...$, the authors constructed a model: 
\begin{align}
	\label{eq:eq1}
	Y_{ij} &= X_i(T_{ij}) + \epsilon_{ij}  \\
	&= \mu(T_{ij}) + \sum_{k=1}^\infty \xi_{ik}\phi_k(T_{ij}) + \epsilon_{ij}, \; \; \;  T_{ij} \in \mathcal{T} 
\end{align}
where $\expec{\epsilon_{ij}} = 0$, var($\epsilon_{ij}$) = $\sigma^2$


\subsection*{Estimation of the Model Components}
Under the assumption that the mean, covariance and eigenfunctions are smooth, the authors first estimated the mean function $\mu$ based on the pooled data from all individuals. {}{}
First, $\mu$ can be estimated by minimizing the following equation \eqref{eq:A.2} respect to $\beta_0$ and $\beta_1$, and obtained as $\hat{\mu}(t) = \hat{\beta_0}(t)$.
Denote kernel functions $\kappa_1: {\rm I\!R} \rightarrow {\rm I\!R} $ and $\kappa_2: {\rm I\!R}^2 \rightarrow {\rm I\!R} $ that satisfy several conditions (omitted here; see appendix of the paper).
\begin{align}
	\label{eq:A.2}
	\sum_{i=1}^{n}\sum_{j=1}^{N_i}\kappa_1(\frac{T_{ij}-t}{h_\mu})\{Y_{ij}-\beta_0-\beta_1(t-T{ij})\}^2{}
\end{align}

% 	\[   
% 	\iint u^{l_1}\nu^{l_2}\kappa_2(u,\nu)dud\nu{} = 
% 	\left\{
% \begin{array}{ll}
%       0 & x\leq l_1+l_2 <l, l_1 \neq \nu_1, l_2 \neq \nu_2 \\
%       (-1)^{|\nu|}|\nu|! & l_1 = \nu_1, l_2 = \nu_2\\
%       \neq 0 & l_1 + l_2 = l\\
% \end{array} 
% 		\right. \]

% where $|\nu| = \nu_1 + \nu_2$, kernel functions $\kappa_1: {\rm I\!R} \rightarrow {\rm I\!R} $ and $\kappa_2: {\rm I\!R}^2 \rightarrow {\rm I\!R} $, 


\begin{align}
	cov(Y_{ij}, Y_{il}) &= cov(X(T_{ij}), X(T_{ij}))+\sigma^2\delta_{ij}
\end{align}
where $\delta_{jl} = 1$ if $j=l$ and $0$ otherwise. 
Denote ``raw'' covariances: $G_i(T_{ij}, T_{il}) = (Y_{ij}-\hat{\mu}(T_{ij}))(Y_{il}-\hat{\mu}(T_{il}))$.
It can be shown that $\expec{\left[G_i(T_{ij}, T_{il})|T_{ij}, T_{il}\right]} \approx cov(X(T_{ij}), X(T_{il})) + \sigma^2 \delta_{jl}$, and thus only $G_i(T_{ij}, T_{il}), j\neq l$ should be included for the covariance surface smoothing step. 
One-curve-leave-out cross-validation is used to select smoothing parameter. 

Denote $\hat{G}(s, t)$ be the estimate of $G(s, t)$. 
The local linear surface smoother for $G(s, t)$ can be estimated by minimizing the following equation \eqref{eq: A.3} with respect to $\pmb{\beta} = (\beta_0, \beta_{11}, \beta_{12})$, yielding estimate $\hat{G}(s, t) = \hat{\beta_0}(s,t)$:

\begin{align}
	\label{eq: A.3}
	\sum_{i=1}^{n}\sum_{1 \leq j \neq l \leq N_i}\kappa_2(\frac{T_{ij}-s}{h_G}, \frac{T_{il}-t}{h_G}) \times \{G_i(T_{ij}, T_{il})-f(\pmb{\beta}, (s, t), (T_{ij}, T_{il}))\}^2
\end{align}
 
\subsection*{Functional Principal Components Analysis Through Conditional Expectation}

\subsection*{Asymptotic Confidence Bands for Individual Trajectories}

\section{ASYMPTOTIC PROPERTIES}

\section{SIMULATION STUDIES}

\section{APPLICATIONS}

\subsection*{Longitudinal CD4 Counts}

\subsection*{Yeast Cell Cycle Gene Expression Profiles}





\end{document}
